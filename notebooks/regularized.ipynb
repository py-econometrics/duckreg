{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda97b40",
   "metadata": {},
   "source": [
    "# Compressed Ridge Regression \n",
    "\n",
    "Comparison with scikit-learn\n",
    "\n",
    "Ridge is computable in closed-form using normal equations:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX + \\lambda I)^{-1}X^\\top y\n",
    "$$\n",
    "\n",
    "We can also show [[ESL](https://hastie.su.domains/ElemStatLearn/) Chapter 3 problem 3.12] that we can solve ridge with the following design matrix\n",
    "\n",
    "$$\n",
    "\\widetilde{X} = \\begin{bmatrix} X \\\\ \\sqrt{\\lambda} I \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "and response vector\n",
    "$$\n",
    "\\widetilde{y} = \\begin{bmatrix} y \\\\ 0_{p \\times s1} \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix and $0_{p \\times 1}$ is a vector of zeros of length $p$ (the number of features). We add $p$ rows to our design matrix, which is typically not a big deal since $p$ is usually much smaller than $n$ in settings we're considering.\n",
    "\n",
    "This lets us use optimized OLS routines [e.g. Lapack drivers such as `gelsd` and `gelsy`] that don't require brittle matrix inversions. This, like Frisch-Waugh-Lovell, might seem peculiar when one encounters it in the classroom -- don't we have fast computers already? But with large datasets, these numerical tricks can make an enormous difference.\n",
    "\n",
    "Importantly, we can use the same trick for compression as OLS used in the rest of the package to solve ridge regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75dbde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import time\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from duckreg.regularized import DuckRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c6abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_large_dataset(N=1_000_000, seed=42):\n",
    "    \"\"\"Generate large synthetic dataset with discrete covariates\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Create discrete covariates (similar to introduction.ipynb)\n",
    "    D = rng.choice([0, 1], size=(N, 1))\n",
    "    f1 = rng.choice(range(3), (N, 1), True)\n",
    "    f2 = rng.choice(range(4), (N, 1), True)\n",
    "    f3 = rng.choice(range(2), (N, 1), True)\n",
    "\n",
    "    # True coefficients\n",
    "    beta_D, beta_f1, beta_f2, beta_f3 = 1.0, 2.0, 1.5, 0.8\n",
    "\n",
    "    # Generate outcome with some noise\n",
    "    Y = (\n",
    "        beta_D * D\n",
    "        + beta_f1 * f1\n",
    "        + beta_f2 * f2\n",
    "        + beta_f3 * f3\n",
    "        + rng.normal(size=(N, 1))\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        np.concatenate([Y, D, f1, f2, f3], axis=1), columns=[\"Y\", \"D\", \"f1\", \"f2\", \"f3\"]\n",
    "    ).assign(rowid=range(N))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_duckdb_database(df, db_name=\"ridge_test.db\", table=\"data\"):\n",
    "    \"\"\"Create and populate DuckDB database\"\"\"\n",
    "    conn = duckdb.connect(db_name)\n",
    "    conn.execute(f\"DROP TABLE IF EXISTS {table}\")\n",
    "    conn.execute(f\"CREATE TABLE {table} AS SELECT * FROM df\")\n",
    "    conn.close()\n",
    "    print(f\"Data loaded into DuckDB database: {db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4494f",
   "metadata": {},
   "source": [
    "### compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff549e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPRESSION EFFECTIVENESS TEST\n",
      "============================================================\n",
      "Data loaded into DuckDB database: compression_test.db\n",
      "Original dataset size: 1,000,000 rows\n",
      "Compressed dataset size: 48 rows\n",
      "Compression ratio: 20833.3x\n",
      "\n",
      "Sample compressed data:\n",
      "     D   f1   f2   f3  count          sum_Y       sum_Y_sq    mean_Y\n",
      "0  0.0  1.0  1.0  0.0  20775   72463.054506  273419.426647  3.487993\n",
      "1  0.0  1.0  0.0  1.0  20928   58647.566319  185433.134264  2.802349\n",
      "2  1.0  0.0  3.0  1.0  20824  130984.656395  844668.731370  6.290081\n",
      "3  1.0  0.0  1.0  0.0  20792   51822.931946  149741.285761  2.492446\n",
      "4  1.0  1.0  2.0  0.0  21132  126700.405564  780735.341177  5.995666\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test how much compression we achieve\"\"\"\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPRESSION EFFECTIVENESS TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = generate_large_dataset(N=1_000_000, seed=42)\n",
    "db_name = \"compression_test.db\"\n",
    "create_duckdb_database(df, db_name)\n",
    "\n",
    "# Check compressed data size\n",
    "duck_ridge = DuckRidge(\n",
    "    db_name=db_name,\n",
    "    table_name=\"data\",\n",
    "    formula=\"Y ~ D + f1 + f2 + f3\",\n",
    "    cv_folds=1,\n",
    "    seed=42,\n",
    ")\n",
    "duck_ridge.prepare_data()\n",
    "duck_ridge.compress_data()\n",
    "\n",
    "original_size = len(df)\n",
    "compressed_size = len(duck_ridge.df_compressed)\n",
    "compression_ratio = original_size / compressed_size\n",
    "\n",
    "print(f\"Original dataset size: {original_size:,} rows\")\n",
    "print(f\"Compressed dataset size: {compressed_size:,} rows\")\n",
    "print(f\"Compression ratio: {compression_ratio:.1f}x\")\n",
    "\n",
    "print(f\"\\nSample compressed data:\")\n",
    "print(duck_ridge.df_compressed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef220ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare DuckRidge vs sklearn Ridge performance\"\"\"\n",
    "print(\"=\" * 60)\n",
    "print(\"DUCKRIDGE VS SKLEARN RIDGE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating synthetic dataset...\")\n",
    "df = generate_large_dataset(N=50_000_000, seed=42)\n",
    "db_name = \"ridge_test.db\"\n",
    "create_duckdb_database(df, db_name)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Sample data:\\n{df.head()}\\n\")\n",
    "\n",
    "# Test different lambda values\n",
    "lambda_values = np.logspace(-5, 10, 10)\n",
    "\n",
    "for lam in lambda_values:\n",
    "    print(f\"\\n--- Testing Î» = {lam} ---\")\n",
    "\n",
    "    # DuckRidge\n",
    "    print(\"Running DuckRidge...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    duck_ridge = DuckRidge(\n",
    "        db_name=db_name,\n",
    "        table_name=\"data\",\n",
    "        formula=\"Y ~ D + f1 + f2 + f3\",\n",
    "        lambda_grid=[lam],  # Single lambda for fair comparison\n",
    "        cv_folds=1,  # No CV for speed\n",
    "        seed=42,\n",
    "    )\n",
    "    duck_ridge.fit(lambda_selection=\"single\")\n",
    "    duck_time = time.time() - start_time\n",
    "    duck_coefs = duck_ridge.point_estimate\n",
    "\n",
    "    print(f\"DuckRidge time: {duck_time:.3f} seconds\")\n",
    "    print(f\"DuckRidge coefficients: {duck_coefs}\")\n",
    "\n",
    "    # Sklearn Ridge (on full data)\n",
    "    print(\"Running sklearn Ridge...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare sklearn data\n",
    "    X_sklearn = df[[\"D\", \"f1\", \"f2\", \"f3\"]].values\n",
    "    y_sklearn = df[\"Y\"].values\n",
    "\n",
    "    sklearn_ridge = Ridge(alpha=lam, fit_intercept=True, solver=\"svd\")\n",
    "    sklearn_ridge.fit(X_sklearn, y_sklearn)\n",
    "    sklearn_time = time.time() - start_time\n",
    "\n",
    "    sklearn_coefs = np.concatenate([[sklearn_ridge.intercept_], sklearn_ridge.coef_])\n",
    "\n",
    "    print(f\"Sklearn time: {sklearn_time:.3f} seconds\")\n",
    "    print(f\"Sklearn coefficients: {sklearn_coefs}\")\n",
    "\n",
    "    # Compare results\n",
    "    speedup = sklearn_time / duck_time\n",
    "    coef_diff = np.abs(duck_coefs - sklearn_coefs)\n",
    "    max_diff = np.max(coef_diff)\n",
    "\n",
    "    print(f\"Speedup: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2986c67",
   "metadata": {},
   "source": [
    "The penalization factor $\\lambda$ is scaled differently in the compressed form, so the coefficients are not identical across the two methods for a given $\\lambda$. However, for a sufficiently fine grid of $\\lambda$ values, we can still find the optimal $\\lambda$ that minimizes the cross-validated error, where compression yields even greater speedups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3472f2",
   "metadata": {},
   "source": [
    "### cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85796398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test DuckRidge cross-validation\"\"\"\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DUCKRIDGE CROSS-VALIDATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use smaller dataset for CV demo\n",
    "print(\"Generating dataset for CV test...\")\n",
    "df = generate_large_dataset(N=10_000_000, seed=42)\n",
    "db_name = \"ridge_cv_test.db\"\n",
    "create_duckdb_database(df, db_name)\n",
    "\n",
    "# Test CV with lambda grid\n",
    "print(\"Running cross-validation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "duck_ridge_cv = DuckRidge(\n",
    "    db_name=db_name,\n",
    "    table_name=\"data\",\n",
    "    formula=\"Y ~ D + f1 + f2 + f3\",\n",
    "    lambda_grid=np.logspace(-3, 1, 20),  # 20 lambda values\n",
    "    cv_folds=5,\n",
    "    seed=42,\n",
    ")\n",
    "duck_ridge_cv.fit(lambda_selection=\"cv\")\n",
    "cv_time = time.time() - start_time\n",
    "\n",
    "print(f\"CV time: {cv_time:.3f} seconds\")\n",
    "print(f\"Best lambda: {duck_ridge_cv.best_lambda:.6f}\")\n",
    "print(f\"Best coefficients: {duck_ridge_cv.point_estimate}\")\n",
    "\n",
    "# Compare with sklearn RidgeCV\n",
    "print(\"\\nRunning sklearn RidgeCV...\")\n",
    "start_time = time.time()\n",
    "\n",
    "X_sklearn = df[[\"D\", \"f1\", \"f2\", \"f3\"]].values\n",
    "y_sklearn = df[\"Y\"].values\n",
    "\n",
    "sklearn_ridge_cv = RidgeCV(alphas=np.logspace(-3, 1, 20), cv=5, fit_intercept=True)\n",
    "sklearn_ridge_cv.fit(X_sklearn, y_sklearn)\n",
    "sklearn_cv_time = time.time() - start_time\n",
    "\n",
    "sklearn_cv_coefs = np.concatenate(\n",
    "    [[sklearn_ridge_cv.intercept_], sklearn_ridge_cv.coef_]\n",
    ")\n",
    "\n",
    "print(f\"Sklearn CV time: {sklearn_cv_time:.3f} seconds\")\n",
    "print(f\"Sklearn best alpha: {sklearn_ridge_cv.alpha_:.6f}\")\n",
    "print(f\"Sklearn coefficients: {sklearn_cv_coefs}\")\n",
    "\n",
    "cv_speedup = sklearn_cv_time / cv_time\n",
    "print(f\"CV Speedup: {cv_speedup:.1f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7976fc",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "============================================================\n",
    "DUCKRIDGE CROSS-VALIDATION TEST\n",
    "============================================================\n",
    "Generating dataset for CV test...\n",
    "Data loaded into DuckDB database: ridge_cv_test.db\n",
    "Running cross-validation...\n",
    "CV time: 0.549 seconds\n",
    "Best lambda: 2.335721\n",
    "Best coefficients: [1.10838176e-03 9.99475426e-01 1.99977411e+00 1.49963835e+00\n",
    " 8.00333145e-01]\n",
    "\n",
    "Running sklearn RidgeCV..\n",
    "\n",
    "Sklearn CV time: 23.304 seconds\n",
    "Sklearn best alpha: 3.792690\n",
    "Sklearn coefficients: [1.10960621e-03 9.99474844e-01 1.99977367e+00 1.49963817e+00\n",
    " 8.00332678e-01]\n",
    "CV Speedup: 27.9x\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
