{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458e4cd0",
   "metadata": {},
   "source": [
    "# Double Machine Learning with Compressed Leave-One-Out Residuals\n",
    "\n",
    "**Theory: From Leave-One-Out Residuals to Compressed Estimation**\n",
    "\n",
    "[Delgado and Mora (1995, Econometrica)](https://e-archivo.uc3m.es/rest/api/core/bitstreams/bedb8220-ea10-4c31-98ae-293acdbbf0c3/content) - the 'nonsmoothing' approach - provides a way to estimate partially linear models using leave-one-out (LOO) residuals when the covariate vector is discrete, as is often the case in applications. Here, we derive an algebraic formulation that allows us to compute the necessary statistics using only group-level aggregates, enabling efficient estimation even with large datasets.\n",
    "\n",
    "Consider the partially linear model:\n",
    "\n",
    "$$Y_i = \\beta X_i + g(Z_i) + U_i$$\n",
    "\n",
    "where we are interested in estimating the parameter $\\beta$. $Z_i$ can be a vector of multiple discrete covariates, which we concatenate into a group indicator. The core idea of the PLM (the most popular DML method going back to Robinson 1988) is to estimate $\\beta$ from a regression of residuals on residuals:\n",
    "\n",
    "$$\\tilde{Y}_i = \\beta \\tilde{X}_i + U_i$$\n",
    "\n",
    "where $\\tilde{Y}_i = Y_i - E[Y | Z_i]$ and $\\tilde{X}_i = X_i - E[X | Z_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc2596",
   "metadata": {},
   "source": [
    "\n",
    "The challenge is estimating the conditional expectations. The Delgado and Mora (1995) nonsmoothing approach uses a **leave-one-out mean** as the estimate. Let's define groups $\\mathcal{G}$ based on unique values of $Z_i$. For an observation $i$ belonging to group $g$, the LOO estimates are:\n",
    "\n",
    "$$\n",
    "\\hat{m}_{y(-i)}(Z_i) = \\frac{1}{N_g - 1} \\sum_{j \\in g, j \\neq i} Y_j \\quad \\text{and} \\quad \\hat{m}_{x(-i)}(Z_i) = \\frac{1}{N_g - 1} \\sum_{j \\in g, j \\neq i} X_j\n",
    "$$\n",
    "\n",
    "The OLS estimator for $\\beta$ is then $\\hat{\\beta} = (\\sum_i \\tilde{X}_i \\tilde{Y}_i) / (\\sum_i \\tilde{X}_i^2)$.\n",
    "\n",
    "**The Algebraic Derivation for Compressed Data**\n",
    "\n",
    "Our goal is to compute $\\sum_i \\tilde{X}_i^2$ and $\\sum_i \\tilde{X}_i \\tilde{Y}_i$ using only group-level aggregates. Let $S_x^{(g)} = \\sum_{j \\in g} X_j$ be the sum of $X$ within group $g$. The LOO mean for observation $i$ in group $g$ can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{m}_{x(-i)} = \\frac{S_x^{(g)} - X_i}{N_g - 1}\n",
    "$$\n",
    "\n",
    "The corresponding residual $\\tilde{X}_i$ for observation $i$ is:\n",
    "\n",
    "$$\n",
    "\\tilde{X}_i = X_i - \\hat{m}_{x(-i)} = X_i - \\frac{S_x^{(g)} - X_i}{N_g - 1} = \\frac{(N_g - 1)X_i - S_x^{(g)} + X_i}{N_g - 1} = \\frac{N_g X_i - S_x^{(g)}}{N_g - 1}\n",
    "$$\n",
    "\n",
    "Now, let's compute the sum of squared residuals **within group g**:\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in g} \\tilde{X}_i^2 = \\sum_{i \\in g} \\left( \\frac{N_g X_i - S_x^{(g)}}{N_g - 1} \\right)^2 = \\frac{1}{(N_g - 1)^2} \\sum_{i \\in g} (N_g^2 X_i^2 - 2 N_g X_i S_x^{(g)} + (S_x^{(g)})^2)\n",
    "$$\n",
    "\n",
    "Distributing the summation:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{(N_g - 1)^2} \\left[ N_g^2 \\sum_{i \\in g} X_i^2 - 2 N_g S_x^{(g)} \\sum_{i \\in g} X_i + \\sum_{i \\in g} (S_x^{(g)})^2 \\right]\n",
    "$$\n",
    "\n",
    "Let $S_{xx}^{(g)} = \\sum_{i \\in g} X_i^2$. We know $\\sum_{i \\in g} X_i = S_x^{(g)}$ and $\\sum_{i \\in g} (S_x^{(g)})^2 = N_g (S_x^{(g)})^2$. Substituting these in:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{(N_g - 1)^2} \\left[ N_g^2 S_{xx}^{(g)} - 2 N_g (S_x^{(g)})^2 + N_g (S_x^{(g)})^2 \\right] = \\frac{N_g}{(N_g - 1)^2} \\left( N_g S_{xx}^{(g)} - (S_x^{(g)})^2 \\right)\n",
    "$$\n",
    "\n",
    "By exact analogy, the sum of cross-products of residuals within group $g$ is:\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in g} \\tilde{X}_i \\tilde{Y}_i = \\frac{N_g}{(N_g - 1)^2} \\left( N_g S_{xy}^{(g)} - S_x^{(g)} S_y^{(g)} \\right)\n",
    "$$\n",
    "\n",
    "The final estimator for $\\beta$ is the ratio of the sum of these quantities across all groups:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\frac{\\sum_{g \\in \\mathcal{G}} \\sum_{i \\in g} \\tilde{X}_i \\tilde{Y}_i}{\\sum_{g \\in \\mathcal{G}} \\sum_{i \\in g} \\tilde{X}_i^2} = \\frac{\\sum_{g \\in \\mathcal{G}} \\frac{N_g}{(N_g - 1)^2} \\left( N_g S_{xy}^{(g)} - S_x^{(g)} S_y^{(g)} \\right)}{\\sum_{g \\in \\mathcal{G}} \\frac{N_g}{(N_g - 1)^2} \\left( N_g S_{xx}^{(g)} - (S_x^{(g)})^2 \\right)}\n",
    "$$\n",
    "\n",
    "This is our master formula. It depends only on group-level counts (`Ng`), sums (`Sx`, `Sy`), and sums of squares/cross-products (`Sxx`, `Sxy`). This is exactly what we can compute with a single SQL `GROUP BY` query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1eb0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from duckreg.estimators import DuckDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955c1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generating sample data...\n",
      "Step 2: Loading 10,000,000 rows into DuckDB table 'sales_data'...\n"
     ]
    }
   ],
   "source": [
    "# --- Simulation Setup ---\n",
    "DB_NAME = \"dml_simulation.db\"\n",
    "TABLE_NAME = \"sales_data\"\n",
    "TRUE_BETA = 2.5\n",
    "\n",
    "# 1. Generate Data\n",
    "print(\"Step 1: Generating sample data...\")\n",
    "N = 10_000_000\n",
    "n_towns = 1000\n",
    "n_days = 100\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'town_id': np.random.randint(0, n_towns, N),\n",
    "    'day_id': np.random.randint(0, n_days, N),\n",
    "})\n",
    "\n",
    "# g(Z) is a non-linear function of town and day\n",
    "g_z = 0.5 * df['town_id'] + 0.01 * df['day_id'] * df['town_id'] + np.sin(df['day_id'])\n",
    "\n",
    "# Treatment X is correlated with the fixed effects\n",
    "df['X'] = 0.2 * df['town_id'] + 0.1 * df['day_id'] + np.random.randn(N)\n",
    "\n",
    "# Outcome Y follows the partially linear model\n",
    "df['Y'] = TRUE_BETA * df['X'] + g_z + np.random.normal(0, 2, N)\n",
    "\n",
    "# 2. Load data into DuckDB\n",
    "print(f\"Step 2: Loading {N:,} rows into DuckDB table '{TABLE_NAME}'...\")\n",
    "con = duckdb.connect(DB_NAME)\n",
    "con.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "con.register('df_pandas', df)\n",
    "con.execute(f\"CREATE TABLE {TABLE_NAME} AS SELECT * FROM df_pandas\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6160fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Initializing and running the DuckPartialLinearLOO estimator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping: 100%|██████████| 500/500 [00:05<00:00, 90.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. Run the DML LOO Estimator\n",
    "print(\"\\nStep 3: Initializing and running the DuckPartialLinearLOO estimator...\")\n",
    "dml_model = DuckDML(\n",
    "    db_name=DB_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    outcome_var='Y',\n",
    "    treatment_var='X',\n",
    "    discrete_covars=['town_id', 'day_id'],\n",
    "    seed=42,\n",
    "    n_bootstraps=500\n",
    ")\n",
    "\n",
    "dml_model.fit()\n",
    "results = dml_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcbe4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Estimation Results ---\n",
      "True Beta: 2.5\n",
      "Estimated Beta: 2.4995\n",
      "Standard Error (Bootstrap): 0.0006\n",
      "--------------------------\n",
      "\n",
      "Original number of rows: 10,000,000\n",
      "Compressed number of rows (groups): 100,000\n"
     ]
    }
   ],
   "source": [
    "# 4. Print Results\n",
    "print(\"\\n--- Estimation Results ---\")\n",
    "print(f\"True Beta: {TRUE_BETA}\")\n",
    "print(f\"Estimated Beta: {results['point_estimate'][0]:.4f}\")\n",
    "print(f\"Standard Error (Bootstrap): {results['standard_error'][[0]].squeeze():.4f}\")\n",
    "print(\"--------------------------\\n\")\n",
    "\n",
    "# Verify the size of the compressed data\n",
    "print(f\"Original number of rows: {len(df):,}\")\n",
    "print(f\"Compressed number of rows (groups): {len(dml_model.df_compressed):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68832a1",
   "metadata": {},
   "source": [
    "This notebook demonstrates the power of combining deep econometric theory with modern data engineering. By leveraging the algebraic properties of the leave-one-out nonsmoothing estimator, we created a Double Machine Learning workflow that:\n",
    "\n",
    "-   **Correctly estimates** the parameter of interest in the presence of high-dimensional fixed effects.\n",
    "-   Is **extremely scalable**, as it operates on a compressed summary of the data that is orders of magnitude smaller than the original dataset.\n",
    "-   **Avoids computational complexity**, replacing expensive k-fold cross-fitting with a single, efficient `GROUP BY` aggregation.\n",
    "\n",
    "This method is a powerful tool for applied researchers who need to control for many discrete covariates without incurring prohibitive computational costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
